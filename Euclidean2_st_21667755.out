Loaded dependency [python3/3.10.12]: gcc/11.4.0-binutils-2.40
Loaded dependency [python3/3.10.12]: sqlite3/3.42.0

Switching from python3/3.8.2 to python3/3.10.12
  Loading requirement: gcc/11.4.0-binutils-2.40 sqlite3/3.42.0
2024-05-02 18:19:48.097952: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-05-02 18:19:48.097990: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-05-02 18:19:48.121325: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
using M.Exp for Logarithm
Epoch: 100 	 loss = -51.2384
Epoch: 200 	 loss = -65.5779
Epoch: 300 	 loss = -47.0074
Epoch: 400 	 loss = -49.3327
Epoch: 500 	 loss = -52.2493
Epoch: 600 	 loss = -51.7272
Epoch: 700 	 loss = -44.9066
Epoch: 800 	 loss = -44.3594
Epoch: 900 	 loss = -66.5643
Epoch: 1000 	 loss = -46.5730
Epoch: 1100 	 loss = -57.6767
Epoch: 1200 	 loss = -36.7245
Epoch: 1300 	 loss = -41.1987
Epoch: 1400 	 loss = -40.7704
Epoch: 1500 	 loss = -48.7014
Epoch: 1600 	 loss = -59.4610
Epoch: 1700 	 loss = -50.6462
Epoch: 1800 	 loss = -39.6529
Epoch: 1900 	 loss = -67.5740
Epoch: 2000 	 loss = -62.0241
Epoch: 2100 	 loss = -61.1753
Epoch: 2200 	 loss = -48.8528
Epoch: 2300 	 loss = -59.2998
Epoch: 2400 	 loss = -63.8177
Epoch: 2500 	 loss = -57.5501
Epoch: 2600 	 loss = -53.4246
Epoch: 2700 	 loss = -85.7484
Epoch: 2800 	 loss = -70.9861
Epoch: 2900 	 loss = -62.1686
Epoch: 3000 	 loss = -54.2337
Epoch: 3100 	 loss = -69.4536
Epoch: 3200 	 loss = -56.6374
Epoch: 3300 	 loss = -65.7627
Epoch: 3400 	 loss = -60.2880
Epoch: 3500 	 loss = -59.8780
Epoch: 3600 	 loss = -60.1346
Epoch: 3700 	 loss = -54.8105
Epoch: 3800 	 loss = -56.0511
Epoch: 3900 	 loss = -57.3629
Epoch: 4000 	 loss = -55.2956
Epoch: 4100 	 loss = -67.6603
Epoch: 4200 	 loss = -70.4469
Epoch: 4300 	 loss = -49.7820
Epoch: 4400 	 loss = -51.6085
Epoch: 4500 	 loss = -50.5331
Epoch: 4600 	 loss = -59.6567
Epoch: 4700 	 loss = -41.9764
Epoch: 4800 	 loss = -60.6316
Epoch: 4900 	 loss = -55.0303
Epoch: 5000 	 loss = -55.5313
Epoch: 5100 	 loss = -57.1378
Epoch: 5200 	 loss = -63.3892
Epoch: 5300 	 loss = -74.8587
Epoch: 5400 	 loss = -76.7028
Epoch: 5500 	 loss = -58.4956
Epoch: 5600 	 loss = -63.3262
Epoch: 5700 	 loss = -55.7657
Epoch: 5800 	 loss = -79.6216
Epoch: 5900 	 loss = -71.6110
Epoch: 6000 	 loss = -52.6178
Epoch: 6100 	 loss = -49.0677
Epoch: 6200 	 loss = -62.4767
Epoch: 6300 	 loss = -57.5532
Epoch: 6400 	 loss = -68.6451
Epoch: 6500 	 loss = -56.5973
Epoch: 6600 	 loss = -64.5002
Epoch: 6700 	 loss = -53.5632
Epoch: 6800 	 loss = -46.6303
Epoch: 6900 	 loss = -61.0598
Epoch: 7000 	 loss = -71.9068
Epoch: 7100 	 loss = -58.6639
Epoch: 7200 	 loss = -57.6675
Epoch: 7300 	 loss = -49.6376
Epoch: 7400 	 loss = -63.6450
Epoch: 7500 	 loss = -59.7798
Epoch: 7600 	 loss = -82.2539
Epoch: 7700 	 loss = -61.5467
Epoch: 7800 	 loss = -58.1842
Epoch: 7900 	 loss = -56.7140
Epoch: 8000 	 loss = -52.4068
Epoch: 8100 	 loss = -72.5639
Epoch: 8200 	 loss = -55.6144
Epoch: 8300 	 loss = -59.8891
Epoch: 8400 	 loss = -50.8416
Epoch: 8500 	 loss = -77.6948
Epoch: 8600 	 loss = -42.2520
Epoch: 8700 	 loss = -64.8320
Epoch: 8800 	 loss = -49.8135
Epoch: 8900 	 loss = -69.7285
Epoch: 9000 	 loss = -53.2918
Epoch: 9100 	 loss = -70.5904
Epoch: 9200 	 loss = -48.0848
Epoch: 9300 	 loss = -41.6013
Epoch: 9400 	 loss = -39.7629
Epoch: 9500 	 loss = -59.2514
Epoch: 9600 	 loss = -57.8218
Epoch: 9700 	 loss = -72.2972
Epoch: 9800 	 loss = -53.2549
Epoch: 9900 	 loss = -54.7503
Epoch: 10000 	 loss = -74.2077
Epoch: 10100 	 loss = -46.5597
Epoch: 10200 	 loss = -73.5561
Epoch: 10300 	 loss = -55.0048
Epoch: 10400 	 loss = -52.1743
Epoch: 10500 	 loss = -63.0639
Epoch: 10600 	 loss = -52.2964
Epoch: 10700 	 loss = -39.7353
Epoch: 10800 	 loss = -71.1111
Epoch: 10900 	 loss = -49.1775
Epoch: 11000 	 loss = -71.7478
Epoch: 11100 	 loss = -69.5279
Epoch: 11200 	 loss = -76.3270
Epoch: 11300 	 loss = -36.0030
Epoch: 11400 	 loss = -68.2544
Epoch: 11500 	 loss = -57.2657
Epoch: 11600 	 loss = -71.2824
Epoch: 11700 	 loss = -69.7637
Epoch: 11800 	 loss = -62.2290
Epoch: 11900 	 loss = -55.5454
Epoch: 12000 	 loss = -67.0057
Epoch: 12100 	 loss = -65.5680
Epoch: 12200 	 loss = -48.6039
Epoch: 12300 	 loss = -53.1391
Epoch: 12400 	 loss = -66.6785
Epoch: 12500 	 loss = -48.4226
Epoch: 12600 	 loss = -67.1780
Epoch: 12700 	 loss = -66.1481
Epoch: 12800 	 loss = -65.2097
Epoch: 12900 	 loss = -74.1132
Epoch: 13000 	 loss = -66.1113
Epoch: 13100 	 loss = -58.4262
Epoch: 13200 	 loss = -70.0224
Epoch: 13300 	 loss = -73.2132
Epoch: 13400 	 loss = -55.3089
Epoch: 13500 	 loss = -62.1184
Epoch: 13600 	 loss = -65.4504
Epoch: 13700 	 loss = -49.7881
Epoch: 13800 	 loss = -65.0129
Epoch: 13900 	 loss = -63.5767
Epoch: 14000 	 loss = -69.0872
Epoch: 14100 	 loss = -56.4270
Epoch: 14200 	 loss = -49.7064
Epoch: 14300 	 loss = -66.3715
Epoch: 14400 	 loss = -62.6019
Epoch: 14500 	 loss = -64.9206
Epoch: 14600 	 loss = -66.4069
Epoch: 14700 	 loss = -81.3178
Epoch: 14800 	 loss = -62.3887
Epoch: 14900 	 loss = -64.8640
Epoch: 15000 	 loss = -80.8339
Epoch: 15100 	 loss = -59.8881
Epoch: 15200 	 loss = -66.1766
Epoch: 15300 	 loss = -56.5349
Epoch: 15400 	 loss = -77.0502
Epoch: 15500 	 loss = -38.5104
Epoch: 15600 	 loss = -45.2099
Epoch: 15700 	 loss = -54.3839
Epoch: 15800 	 loss = -61.7121
Epoch: 15900 	 loss = -68.7609
Epoch: 16000 	 loss = -44.1740
Epoch: 16100 	 loss = -50.1693
Epoch: 16200 	 loss = -76.0063
Epoch: 16300 	 loss = -60.4471
Epoch: 16400 	 loss = -54.4196
Epoch: 16500 	 loss = -84.7640
Epoch: 16600 	 loss = -64.3666
Epoch: 16700 	 loss = -48.2890
Epoch: 16800 	 loss = -56.9598
Epoch: 16900 	 loss = -47.5970
Epoch: 17000 	 loss = -61.3972
Epoch: 17100 	 loss = -72.6415
Epoch: 17200 	 loss = -61.0693
Epoch: 17300 	 loss = -51.4930
Epoch: 17400 	 loss = -61.3798
Epoch: 17500 	 loss = -65.7443
Epoch: 17600 	 loss = -65.3472
Epoch: 17700 	 loss = -51.3543
Epoch: 17800 	 loss = -50.1138
Epoch: 17900 	 loss = -65.9197
Epoch: 18000 	 loss = -52.7582
Epoch: 18100 	 loss = -57.8200
Epoch: 18200 	 loss = -55.3263
Epoch: 18300 	 loss = -66.8676
Epoch: 18400 	 loss = -63.4638
Epoch: 18500 	 loss = -54.7705
Epoch: 18600 	 loss = -51.2418
Epoch: 18700 	 loss = -87.4062
Epoch: 18800 	 loss = -76.9548
Epoch: 18900 	 loss = -53.5134
Epoch: 19000 	 loss = -53.8477
Epoch: 19100 	 loss = -62.6041
Epoch: 19200 	 loss = -63.4112
Epoch: 19300 	 loss = -67.3962
Epoch: 19400 	 loss = -71.0656
Epoch: 19500 	 loss = -52.7416
Traceback (most recent call last):
  File "/zhome/77/8/118225/Desktop/Projects/score_diffusion_mean/score_diffusion_mean/train_t.py", line 170, in <module>
    train_score()
  File "/zhome/77/8/118225/Desktop/Projects/score_diffusion_mean/score_diffusion_mean/train_t.py", line 149, in train_score
    train_t(M=M,
  File "/zhome/77/8/118225/Desktop/Projects/score_diffusion_mean/score_diffusion_mean/jaxgeometry/statistics/score_matching/traint.py", line 119, in train_t
    if ((not any(jnp.sum(jnp.isnan(val))>0 for val in new_state.params[list(new_state.params.keys())[0]].values())) \
  File "/zhome/77/8/118225/.local/lib/python3.10/site-packages/jax/_src/array.py", line 262, in __bool__
    return bool(self._value)
  File "/zhome/77/8/118225/.local/lib/python3.10/site-packages/jax/_src/profiler.py", line 340, in wrapper
    return func(*args, **kwargs)
  File "/zhome/77/8/118225/.local/lib/python3.10/site-packages/jax/_src/array.py", line 566, in _value
    self._npy_value = self._single_device_array_to_np_array()  # type: ignore
KeyboardInterrupt
terminate called without an active exception
/zhome/77/8/118225/.lsbatch/1714631245.21667755.shell: line 30: 26566 Aborted                 python3 train_t.py --manifold Sphere --dim 2 --load_model 0 --max_T 1.0 --lr_rate 0.0002 --epochs 50000 --x_samples 64 --repeats 16 --samples_per_batch 32 --dt_steps 100 --save_step 100 --seed 2712
