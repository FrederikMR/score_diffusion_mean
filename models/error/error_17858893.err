Loaded dependency [python3/3.9.11]: gcc/10.3.0-binutils-2.36.1

Switching from python3/3.8.2 to python3/3.9.11
  Loading requirement: gcc/10.3.0-binutils-2.36.1
2023-09-26 08:01:20.443847: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1695708097.768897   16746 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
2023-09-26 08:01:40.078830: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
/zhome/77/8/118225/.local/lib/python3.9/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.
  setattr(self, word, getattr(machar, word).flat[0])
/zhome/77/8/118225/.local/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.
  return self._float_to_str(self.smallest_subnormal)
2023-09-26 08:01:47.496513: E external/xla/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:

  %reduce-window = f32[32768,1,4,4]{3,2,1,0} reduce-window(f32[32768,1,100,100]{3,2,1,0} %broadcast.212, f32[] %constant.26), window={size=1x1x32x32 stride=1x1x32x32 pad=0_0x0_0x14_14x14_14}, to_apply=%region_4.1012

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2023-09-26 08:04:14.642708: E external/xla/xla/service/slow_operation_alarm.cc:133] The operation took 2m28.146345107s
Constant folding an instruction is taking > 1s:

  %reduce-window = f32[32768,1,4,4]{3,2,1,0} reduce-window(f32[32768,1,100,100]{3,2,1,0} %broadcast.212, f32[] %constant.26), window={size=1x1x32x32 stride=1x1x32x32 pad=0_0x0_0x14_14x14_14}, to_apply=%region_4.1012

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Traceback (most recent call last):
  File "/zhome/77/8/118225/Desktop/smrdm/smrdm/train_score.py", line 307, in <module>
    train_rn_s2(file_path = 'models/R', 
  File "/zhome/77/8/118225/Desktop/smrdm/smrdm/train_score.py", line 280, in train_rn_s2
    train_s2(M=M,
  File "/zhome/77/8/118225/Desktop/smrdm/smrdm/jaxgeometry/statistics/score_matching/trainxt.py", line 261, in train_s2
    state, loss_val = update(state, next(train_dataset))
  File "<string>", line 1, in <lambda>
KeyboardInterrupt
2023-09-26 09:43:17.046830: W tensorflow/core/kernels/data/generator_dataset_op.cc:108] Error occurred when finalizing GeneratorDataset iterator: FAILED_PRECONDITION: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
